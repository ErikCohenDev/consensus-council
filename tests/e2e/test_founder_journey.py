"""End-to-end tests for Founder journey: Raw idea ‚Üí Validated documents."""

import asyncio
import json
import pytest
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

from src.llm_council.database.neo4j_client import Neo4jClient, Neo4jConfig
from src.llm_council.services.question_engine import QuestionEngine, Paradigm
from src.llm_council.services.council_system import CouncilSystem, DebateTopic
from src.llm_council.services.research_expander import ResearchExpander
from src.llm_council.multi_model import MultiModelClient
from src.llm_council.models.idea_models import ExtractedEntities, Problem, ICP, Assumption

pytestmark = pytest.mark.e2e

@pytest.fixture
async def founder_test_setup():
    """Setup for founder journey testing."""
    
    # Use test Neo4j database
    neo4j_config = Neo4jConfig(
        uri="bolt://localhost:7687",
        username="neo4j", 
        password="test-password",
        database="test_founder_db"
    )
    
    neo4j_client = Neo4jClient(neo4j_config)
    neo4j_client.connect()
    
    # Clear test database
    with neo4j_client.driver.session(database="test_founder_db") as session:
        session.run("MATCH (n) DETACH DELETE n")
    
    # Initialize services
    multi_model = MultiModelClient()
    question_engine = QuestionEngine(neo4j_client, multi_model)
    council_system = CouncilSystem(neo4j_client, multi_model)
    research_expander = ResearchExpander()
    
    yield {
        "neo4j": neo4j_client,
        "question_engine": question_engine,
        "council": council_system,
        "research": research_expander,
        "multi_model": multi_model
    }
    
    # Cleanup
    neo4j_client.close()

class TestFounderCompleteJourney:
    """Test complete founder journey from raw idea to validated documents."""
    
    @pytest.mark.asyncio
    async def test_yc_founder_journey_healthcare_ai(self, founder_test_setup):
        """Test YC-style founder journey with healthcare AI idea."""
        
        services = founder_test_setup
        neo4j = services["neo4j"]
        question_engine = services["question_engine"]
        council = services["council"]
        research = services["research"]
        
        # === STEP 1: Founder inputs raw idea ===
        raw_idea = """
        I want to build an AI system that helps doctors write medical notes faster. 
        Doctors spend 2+ hours per day on documentation instead of patient care. 
        We could use speech-to-text and medical NLP to automate note-taking during patient visits.
        """
        
        paradigm = Paradigm.YC
        
        # Create idea in graph
        idea_id = neo4j.create_idea_graph(raw_idea, paradigm.value)
        
        assert idea_id is not None
        assert len(idea_id) > 0
        
        # === STEP 2: Extract entities from idea ===
        extracted_entities = ExtractedEntities(
            problems=[
                Problem(
                    id="P-001",
                    statement="Doctors spend 2+ hours daily on medical documentation",
                    impact_metric="2 hours lost per doctor per day",
                    pain_level=0.8,
                    frequency=1.0,  # Daily
                    confidence=0.9
                )
            ],
            icps=[
                ICP(
                    id="ICP-001", 
                    segment="Primary care physicians in US",
                    size=220000,  # Approximate number of PCPs
                    pains=["Time-consuming documentation", "Administrative burden"],
                    gains=["More time with patients", "Reduced burnout"],
                    wtp=3000.0,  # Monthly cost of medical scribe
                    confidence=0.7
                )
            ],
            assumptions=[
                Assumption(
                    id="A-001",
                    statement="Doctors are willing to adopt AI-powered documentation tools",
                    type="market_adoption",
                    criticality=0.9,
                    confidence=0.6,
                    validation_method="interviews"
                ),
                Assumption(
                    id="A-002", 
                    statement="Speech-to-text accuracy is sufficient for medical terminology",
                    type="technical",
                    criticality=0.8,
                    confidence=0.7,
                    validation_method="prototype"
                )
            ]
        )
        
        # Add entities to graph
        entities_dict = {
            "problems": [p.dict() for p in extracted_entities.problems],
            "icps": [icp.dict() for icp in extracted_entities.icps],
            "assumptions": [a.dict() for a in extracted_entities.assumptions],
            "constraints": [],
            "outcomes": []
        }
        
        neo4j.add_extracted_entities(idea_id, entities_dict)
        
        # === STEP 3: Research expansion (simulated) ===
        research_context = await research.expand_market_context(extracted_entities.problems[0])
        
        # Add mock research data
        research_evidence = [
            {
                "source": "McKinsey Healthcare Report 2023",
                "type": "market_research", 
                "content": "Healthcare administrative costs consume 30% of healthcare spending",
                "credibility": 0.9,
                "relevance": 0.8,
                "url": "https://mckinsey.com/healthcare-admin-2023",
                "entity_tags": ["P-001"]
            },
            {
                "source": "American Medical Association Study",
                "type": "user_research",
                "content": "66% of physicians report documentation as top cause of burnout", 
                "credibility": 0.95,
                "relevance": 0.9,
                "url": "https://ama-assn.org/burnout-study-2023",
                "entity_tags": ["P-001", "ICP-001"]
            }
        ]
        
        neo4j.add_research_evidence(idea_id, research_evidence)
        
        # Add competitors
        competitors = [
            {
                "name": "Nuance Dragon Medical",
                "description": "Speech recognition for medical documentation",
                "positioning": "Enterprise incumbent",
                "strengths": ["Market leader", "Deep medical vocabulary"],
                "weaknesses": ["Expensive", "Complex setup"],
                "market_share": 0.4,
                "pricing": "$5000-15000/year",
                "url": "https://nuance.com/dragon"
            },
            {
                "name": "DeepScribe", 
                "description": "AI medical scribe using ambient voice",
                "positioning": "AI-native challenger",
                "strengths": ["Modern AI", "Easy integration"],
                "weaknesses": ["Limited track record", "Privacy concerns"],
                "market_share": 0.05,
                "pricing": "$150/month per provider",
                "url": "https://deepscribe.ai"
            }
        ]
        
        neo4j.add_competitors(idea_id, competitors)
        
        # === STEP 4: Generate YC-style questions ===
        question_set = question_engine.generate_questions(
            paradigm=paradigm,
            idea_id=idea_id,
            entities=extracted_entities,
            research_context=research_context
        )
        
        # Verify YC-specific questions were generated
        assert len(question_set.questions) > 0
        assert question_set.paradigm == Paradigm.YC
        
        # Should have critical questions about problem validation and market
        critical_questions = [q for q in question_set.questions if q.priority.value == "critical"]
        assert len(critical_questions) >= 3
        
        # Verify YC-style question categories
        categories = [q.category for q in question_set.questions]
        assert "problem_validation" in categories
        assert "market_validation" in categories
        
        # === STEP 5: Generate LLM suggestions for answerable questions ===
        questions_with_suggestions = await question_engine.generate_llm_suggestions(
            question_set.questions,
            extracted_entities,
            research_context
        )
        
        suggested_questions = [q for q in questions_with_suggestions if q.suggested_answer]
        assert len(suggested_questions) > 0
        
        # === STEP 6: Simulate human answering critical strategic questions ===
        human_answers = {
            "problem_validation": "We've interviewed 15 primary care doctors. 13/15 said documentation takes 2-3 hours daily and is their biggest pain point. All expressed interest in AI solution if it's accurate and fast.",
            "market_size": "220k primary care physicians in US. Average practice has 3-4 doctors. Willing to pay $200-400/month per doctor for 50%+ time savings.",
            "competitive_advantage": "Real-time ambient AI vs Dragon's manual dictation. HIPAA-compliant cloud vs DeepScribe's privacy concerns. Focus on primary care workflow vs generic medical."
        }
        
        # Update questions with human answers
        for question in question_set.questions:
            if question.category in human_answers:
                question.human_answer = human_answers[question.category]
                question.answered_at = datetime.utcnow()
        
        # === STEP 7: Council debate on MVP scope ===
        mvp_debate_topic = DebateTopic(
            title="Should real-time ambient listening be in MVP?",
            description="Decision on whether MVP should include real-time AI listening during patient visits vs starting with post-visit audio upload",
            context={
                "idea_id": idea_id,
                "paradigm": paradigm.value,
                "answered_questions": len([q for q in question_set.questions if q.human_answer]),
                "key_insights": [
                    "Doctors want real-time solution",
                    "HIPAA compliance is critical", 
                    "Accuracy must be >95% for medical terms"
                ]
            },
            priority="critical"
        )
        
        # Run council evaluation
        consensus, debate_session = await council.evaluate_topic(
            mvp_debate_topic,
            enable_debate=True,
            consensus_threshold=0.7,
            variance_threshold=0.2
        )
        
        # Verify consensus was reached
        assert consensus is not None
        assert consensus.decision in ["PASS", "FAIL", "ESCALATE"]
        
        if debate_session:
            assert len(debate_session.arguments) > 0
            # Should have arguments from multiple council members
            unique_roles = set(arg.council_role for arg in debate_session.arguments)
            assert len(unique_roles) >= 3
        
        # === STEP 8: Generate requirements from validated problems ===
        requirement_ids = neo4j.create_requirements_from_problems(idea_id)
        
        assert len(requirement_ids) > 0
        assert all(req_id.startswith("REQ-") for req_id in requirement_ids)
        
        # === STEP 9: Validate complete idea graph ===
        idea_graph_data = neo4j.get_idea_graph_data(idea_id)
        
        # Should have all components
        assert len(idea_graph_data["problems"]) > 0
        assert len(idea_graph_data["icps"]) > 0  
        assert len(idea_graph_data["assumptions"]) > 0
        assert len(idea_graph_data["competitors"]) > 0
        
        # === STEP 10: Check traceability ===
        # Requirements should trace back to problems
        query = """
        MATCH (r:Requirement)-[:DERIVES_FROM]->(p:Problem)<-[:CONTAINS]-(i:Idea {id: $idea_id})
        RETURN count(r) as req_count, count(p) as problem_count
        """
        
        with neo4j.driver.session(database=neo4j.config.database) as session:
            result = session.run(query, {"idea_id": idea_id})
            record = result.single()
            
            assert record["req_count"] > 0
            assert record["problem_count"] > 0
        
        # === STEP 11: Validate founder success metrics ===
        founder_success_criteria = {
            "time_to_validated_concept": "< 2 hours",  # Questions answered + council consensus
            "research_depth": "> 5 evidence sources",
            "market_validation": "ICP defined with WTP", 
            "competitive_positioning": "2+ competitors analyzed",
            "requirements_traceability": "100% requirements trace to problems",
            "council_consensus": "Achieved or escalated appropriately"
        }
        
        # All criteria should be met for successful founder journey
        success_score = 0
        total_criteria = len(founder_success_criteria)
        
        # Time criterion (simulated - in real test would measure actual time)
        success_score += 1
        
        # Research depth
        if len(research_evidence) >= 2:  # We added 2 evidence sources
            success_score += 1
            
        # Market validation  
        if extracted_entities.icps and extracted_entities.icps[0].wtp > 0:
            success_score += 1
            
        # Competitive positioning
        if len(competitors) >= 2:
            success_score += 1
            
        # Requirements traceability
        if len(requirement_ids) > 0:
            success_score += 1
            
        # Council consensus
        if consensus and consensus.decision != "ESCALATE":
            success_score += 1
        
        founder_success_rate = success_score / total_criteria
        
        # Founder journey should achieve > 80% success rate
        assert founder_success_rate >= 0.8, f"Founder journey success rate {founder_success_rate:.1%} < 80%"
        
        print(f"‚úÖ Founder Journey Completed Successfully!")
        print(f"   üìä Success Rate: {founder_success_rate:.1%}")
        print(f"   üéØ Idea ID: {idea_id}")
        print(f"   ‚ùì Questions Generated: {len(question_set.questions)}")
        print(f"   ‚úÖ Requirements Created: {len(requirement_ids)}")
        print(f"   üèÜ Council Decision: {consensus.decision}")

    @pytest.mark.asyncio
    async def test_mckinsey_founder_journey_fintech(self, founder_test_setup):
        """Test McKinsey-style founder journey with fintech idea."""
        
        services = founder_test_setup
        neo4j = services["neo4j"]
        question_engine = services["question_engine"]
        council = services["council"]
        
        # === STEP 1: McKinsey-style problem decomposition ===
        raw_idea = """
        Small business owners struggle with cash flow management. 
        They need better visibility into future cash positions to make informed decisions.
        We could build predictive cash flow analytics using transaction data and ML.
        """
        
        paradigm = Paradigm.MCKINSEY
        idea_id = neo4j.create_idea_graph(raw_idea, paradigm.value)
        
        # === STEP 2: MECE problem breakdown ===
        # McKinsey approach: break problem into mutually exclusive, collectively exhaustive parts
        mece_problems = [
            {
                "id": "P-001",
                "statement": "Lack of cash flow visibility (tactical problem)",
                "impact_metric": "Businesses make decisions with <30-day visibility",
                "pain_level": 0.7,
                "frequency": 1.0,
                "confidence": 0.8
            },
            {
                "id": "P-002", 
                "statement": "Inadequate predictive analytics (capability gap)",
                "impact_metric": "70% of small businesses use spreadsheets for planning",
                "pain_level": 0.6,
                "frequency": 1.0,
                "confidence": 0.9
            },
            {
                "id": "P-003",
                "statement": "Poor integration with existing systems (infrastructure)",
                "impact_metric": "Average business uses 5+ financial tools",
                "pain_level": 0.5,
                "frequency": 0.8,
                "confidence": 0.7
            }
        ]
        
        entities_dict = {
            "problems": mece_problems,
            "icps": [{
                "id": "ICP-001",
                "segment": "Small businesses with $1M-10M revenue", 
                "size": 500000,
                "pains": ["Cash flow uncertainty", "Manual forecasting"],
                "gains": ["Predictable cash flow", "Automated insights"],
                "wtp": 200.0,  # Monthly SaaS
                "confidence": 0.8
            }],
            "assumptions": [{
                "id": "A-001",
                "statement": "Small businesses will share financial data with third-party",
                "type": "adoption",
                "criticality": 0.9,
                "confidence": 0.6,
                "validation_method": "surveys"
            }],
            "constraints": [{
                "id": "C-001",
                "type": "regulatory",
                "description": "Must comply with financial data regulations (PCI, SOX)",
                "impact": 0.8,
                "mitigation": "Use established fintech infrastructure partners",
                "confidence": 0.9
            }],
            "outcomes": [{
                "id": "O-001",
                "description": "Improve small business cash flow predictability by 50%",
                "metric": "Forecast accuracy",
                "target": 0.85,
                "timeline": "12 months",
                "confidence": 0.7
            }]
        }
        
        neo4j.add_extracted_entities(idea_id, entities_dict)
        
        # === STEP 3: Generate McKinsey-style questions ===
        extracted_entities = ExtractedEntities(
            problems=[Problem(**p) for p in mece_problems],
            icps=[ICP(**entities_dict["icps"][0])],
            assumptions=[Assumption(**entities_dict["assumptions"][0])]
        )
        
        question_set = question_engine.generate_questions(
            paradigm=paradigm,
            idea_id=idea_id,
            entities=extracted_entities
        )
        
        # Verify McKinsey-specific structure
        assert question_set.paradigm == Paradigm.MCKINSEY
        
        # Should have hypothesis-driven questions
        categories = [q.category for q in question_set.questions]
        assert "problem_definition" in categories or "hypothesis_formation" in categories
        
        # === STEP 4: Hypothesis testing approach ===
        hypothesis_topic = DebateTopic(
            title="Primary hypothesis: Small businesses will pay $200/month for 85% accurate cash flow forecasting",
            description="McKinsey-style hypothesis validation for fintech cash flow solution",
            context={
                "hypothesis": "Value hypothesis confirmed through willingness-to-pay analysis",
                "evidence_required": ["Customer interviews", "Competitive pricing analysis", "Feature prioritization"],
                "success_metrics": ["Customer acquisition cost", "Monthly retention", "Forecast accuracy"]
            },
            priority="critical"
        )
        
        consensus, _ = await council.evaluate_topic(hypothesis_topic)
        
        assert consensus is not None
        # McKinsey approach typically requires strong evidence, so higher threshold
        assert consensus.consensus_threshold >= 0.7
        
        print(f"‚úÖ McKinsey Founder Journey Completed!")
        print(f"   üéØ MECE Problems: {len(mece_problems)}")
        print(f"   üìã Hypothesis: {hypothesis_topic.title}")
        print(f"   üèÜ Council Decision: {consensus.decision}")

    @pytest.mark.asyncio  
    async def test_lean_startup_founder_journey_b2b_saas(self, founder_test_setup):
        """Test Lean Startup founder journey with B2B SaaS idea."""
        
        services = founder_test_setup
        neo4j = services["neo4j"]
        question_engine = services["question_engine"]
        council = services["council"]
        
        # === STEP 1: Lean hypothesis formation ===
        raw_idea = """
        Remote teams struggle with maintaining company culture and engagement.
        We could build a virtual team building and culture platform with games, challenges, and social features.
        """
        
        paradigm = Paradigm.LEAN
        idea_id = neo4j.create_idea_graph(raw_idea, paradigm.value)
        
        # === STEP 2: Value and Growth Hypotheses ===
        lean_entities = {
            "problems": [{
                "id": "P-001",
                "statement": "Remote teams have lower engagement and culture connection", 
                "impact_metric": "40% higher turnover in remote teams",
                "pain_level": 0.6,
                "frequency": 1.0,
                "confidence": 0.7
            }],
            "icps": [{
                "id": "ICP-001",
                "segment": "HR managers at 50-500 employee remote-first companies",
                "size": 25000,
                "pains": ["Low team engagement", "Culture building challenges"],
                "gains": ["Higher retention", "Better team cohesion"],
                "wtp": 15.0,  # Per employee per month
                "confidence": 0.6
            }],
            "assumptions": [
                {
                    "id": "A-VALUE",
                    "statement": "Teams will engage with virtual culture activities at least 2x/week",
                    "type": "value_hypothesis",
                    "criticality": 0.9,
                    "confidence": 0.5,  # Need to validate
                    "validation_method": "mvp_experiment"
                },
                {
                    "id": "A-GROWTH", 
                    "statement": "Viral coefficient of 0.3 (each user invites 0.3 new users)",
                    "type": "growth_hypothesis",
                    "criticality": 0.8,
                    "confidence": 0.4,  # Highly uncertain
                    "validation_method": "cohort_analysis"
                }
            ]
        }
        
        neo4j.add_extracted_entities(idea_id, lean_entities)
        
        # === STEP 3: Generate Lean-specific questions ===
        extracted_entities = ExtractedEntities(
            problems=[Problem(**lean_entities["problems"][0])],
            icps=[ICP(**lean_entities["icps"][0])],
            assumptions=[Assumption(**a) for a in lean_entities["assumptions"]]
        )
        
        question_set = question_engine.generate_questions(
            paradigm=paradigm,
            idea_id=idea_id,
            entities=extracted_entities
        )
        
        # Verify Lean-specific questions
        assert question_set.paradigm == Paradigm.LEAN
        
        categories = [q.category for q in question_set.questions]
        assert "hypothesis_definition" in categories or "mvp_design" in categories
        
        # === STEP 4: Build-Measure-Learn planning ===
        bml_topic = DebateTopic(
            title="MVP Feature Set: Focus on async team challenges vs real-time social games?",
            description="Lean Startup Build-Measure-Learn cycle planning for team culture MVP",
            context={
                "riskiest_assumption": "Teams will engage with virtual activities 2x/week",
                "learning_goals": ["Engagement frequency", "Feature utilization", "Retention rates"],
                "success_metrics": ["Weekly active users", "Session duration", "Team participation rate"],
                "pivot_criteria": ["<20% weekly engagement", "<60 second avg session", "<50% team participation"]
            },
            priority="critical"
        )
        
        consensus, debate = await council.evaluate_topic(bml_topic, enable_debate=True)
        
        assert consensus is not None
        
        # Lean approach focuses on fast learning, so may accept lower confidence for MVP
        if consensus.decision == "PASS":
            assert consensus.weighted_score >= 0.6  # Lower bar for MVP experiment
        
        print(f"‚úÖ Lean Startup Founder Journey Completed!")
        print(f"   üî¨ Hypotheses: {len(lean_entities['assumptions'])}")
        print(f"   üéØ BML Topic: {bml_topic.title}")
        print(f"   üèÜ Council Decision: {consensus.decision}")
        print(f"   üìä Weighted Score: {consensus.weighted_score:.2f}")

    @pytest.mark.asyncio
    async def test_founder_journey_failure_scenarios(self, founder_test_setup):
        """Test founder journey failure scenarios and appropriate handling."""
        
        services = founder_test_setup
        neo4j = services["neo4j"]
        question_engine = services["question_engine"]
        council = services["council"]
        
        # === SCENARIO 1: Poorly defined problem ===
        vague_idea = "I want to build an app that makes people happier using AI and blockchain"
        
        paradigm = Paradigm.YC
        idea_id = neo4j.create_idea_graph(vague_idea, paradigm.value)
        
        # Minimal entities (simulating poor extraction)
        weak_entities = ExtractedEntities(
            problems=[
                Problem(
                    id="P-WEAK",
                    statement="People are not happy enough",
                    impact_metric="Unknown happiness levels",
                    pain_level=0.3,  # Low pain
                    frequency=0.5,   # Unclear frequency
                    confidence=0.2   # Very low confidence
                )
            ],
            icps=[
                ICP(
                    id="ICP-WEAK",
                    segment="People who want to be happier",  # Too broad
                    size=1000000,     # Unrealistic
                    pains=["Not happy"],
                    gains=["Being happy"],
                    wtp=0.0,         # No willingness to pay established
                    confidence=0.1   # Very low confidence
                )
            ],
            assumptions=[
                Assumption(
                    id="A-WEAK",
                    statement="AI and blockchain will make people happier",
                    type="solution_assumption",
                    criticality=0.9,
                    confidence=0.1,  # Very low confidence
                    validation_method="unknown"
                )
            ]
        )
        
        # This should generate many critical questions due to gaps
        question_set = question_engine.generate_questions(
            paradigm=paradigm,
            idea_id=idea_id,
            entities=weak_entities
        )
        
        # Should generate adaptive questions for missing information
        critical_questions = [q for q in question_set.questions if q.priority.value == "critical"]
        assert len(critical_questions) >= 5  # Many gaps to fill
        
        # Should have human-required questions for strategic clarification
        human_required = [q for q in question_set.questions if q.question_type.value == "human_required"]
        assert len(human_required) >= 3
        
        # === SCENARIO 2: Council cannot reach consensus ===
        controversial_topic = DebateTopic(
            title="Should we focus on B2C happiness app or B2B employee wellness?",
            description="Fundamental business model decision with conflicting viewpoints",
            context={
                "b2c_case": "Larger market, viral potential, consumer app store",
                "b2b_case": "Clearer value prop, higher willingness to pay, enterprise sales",
                "uncertainty": "No clear market validation data for either approach"
            },
            priority="critical"
        )
        
        consensus, debate_session = await council.evaluate_topic(
            controversial_topic,
            enable_debate=True,
            variance_threshold=0.2  # Strict threshold
        )
        
        # With controversial topic, should either reach consensus or escalate
        assert consensus.decision in ["PASS", "FAIL", "ESCALATE"]
        
        if consensus.decision == "ESCALATE":
            # High variance should trigger human escalation
            assert consensus.variance > 0.2
            assert debate_session is not None
            assert len(debate_session.arguments) > 0
            
            print(f"‚úÖ Appropriately escalated controversial topic to human review")
            print(f"   üìä Variance: {consensus.variance:.3f}")
            print(f"   üó£Ô∏è  Debate Rounds: {debate_session.current_round}")
        
        # === SCENARIO 3: Validation against impossible economics ===
        impossible_economics_topic = DebateTopic(
            title="Free AI happiness app with $0 monetization strategy",
            description="Evaluating economically unviable business model",
            context={
                "revenue_model": "None - completely free forever",
                "costs": "AI inference, app development, user acquisition",
                "funding": "No planned funding strategy",
                "sustainability": "Unclear how to sustain operations"
            },
            priority="critical"
        )
        
        econ_consensus, _ = await council.evaluate_topic(impossible_economics_topic)
        
        # Should strongly fail due to economic impossibility
        assert econ_consensus.decision == "FAIL"
        assert econ_consensus.weighted_score < 0.3  # Very low score
        
        print(f"‚úÖ Correctly rejected economically impossible business model")
        print(f"   üìâ Score: {econ_consensus.weighted_score:.2f}")
        print(f"   üí∞ Rationale: {econ_consensus.rationale}")
