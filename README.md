# LLM Council Audit & Consensus Platform

From idea ‚Üí plan ‚Üí implementation.

## üìã Document Workflow (Sequential Gates)

**Research Phase:**

- **[RESEARCH_BRIEF.md](./docs/RESEARCH_BRIEF.md)** ‚Äî Problem validation, hypothesis, research methodology
- **[MARKET_SCAN.md](./docs/MARKET_SCAN.md)** ‚Äî Competitive analysis, market opportunity, BUILD/BUY/PARTNER decision

**Design Phase:**

- **[VISION.md](./docs/VISION.md)** ‚Äî Why this matters, who it serves, MVP scope and success
- **[PRD.md](./docs/PRD.md)** ‚Äî Requirements (with `R-PRD-###` IDs), acceptance criteria, NFRs, eval plan
- **[ARCHITECTURE.md](./docs/ARCHITECTURE.md)** ‚Äî Design to satisfy the PRD (components, data, interfaces, SLOs, security, diagrams)

**Implementation Phase:**

- **[IMPLEMENTATION_PLAN.md](./docs/IMPLEMENTATION_PLAN.md)** ‚Äî Task breakdown with `T-###`, TDD-ready, owners/estimates, traceability back to PRD
- **[AUDITOR_SCHEMA.md](./docs/AUDITOR_SCHEMA.md)** ‚Äî LLM council structure, rubric, consensus algorithm, quality gates
- **[HUMAN_REVIEW_INTERFACE.md](./docs/HUMAN_REVIEW_INTERFACE.md)** ‚Äî Human-in-the-loop design for strategic decisions and consensus deadlocks

## üö™ Quality Gates

Each document must pass consensus from our LLM council (PM, Infrastructure, Data/Eval, Security, UX, Cost auditors) before proceeding. **Strategic documents require human review**, while technical documents can auto-approve with high consensus:

- **Research Brief ‚Üí Market Scan:** Problem validated, research plan approved _(human review)_
- **Market Scan ‚Üí Vision:** BUILD decision justified, market opportunity clear _(human review)_
- **Vision ‚Üí PRD:** Success metrics quantified, MVP scope realistic _(human review)_
- **PRD ‚Üí Architecture:** All requirements mapped, no critical gaps _(human review)_
- **Architecture ‚Üí Implementation:** Design complete, tasks traceable _(auto-approve if consensus)_

**Human Review Triggers:**

- Strategic documents (Research, Market, Vision, PRD)
- Low consensus between auditors (stdev > 1.0)
- Blocking issues detected
- Consensus deadlock after 3 attempts

## üõ†Ô∏è Setup

1. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment:**

   ```bash
   export OPENAI_API_KEY="your-key-here"
   # Optional: export ANTHROPIC_API_KEY="your-key-here"
   ```

3. **Initialize your project documents:**

   ```bash
   python scripts/init_docs.py --project "Your Project" --owner "Your Name"
   # Or run interactively: python scripts/init_docs.py
   ```

4. **Run the LLM council audit (when implemented):**

   ```bash
   # Interactive mode (human review for strategic docs)
   python audit.py ./docs --stage vision --interactive

   # Auto mode (skip human review, fail on disagreement)
   python audit.py ./docs --stage architecture --auto-approve

   # Full pipeline with human checkpoints
   python audit.py ./docs --full-pipeline --interactive
   ```

> Keep each file brief. If a section is not applicable, write **N/A (why)** rather than leaving it blank.
> Use markdown features (tables, task lists, code blocks) for clarity.

## üåê Web UI

Two ways to run the UI:

- Development (recommended):
  - Backend: `python -m src.llm_council.ui_server`
  - Frontend: `cd frontend && npm install && npm run dev` (Vite dev server at `http://localhost:3000`, proxying `/api` and `/ws` to `http://localhost:8000`)

- Production-like:
  - Build: `cd frontend && npm install && npm run build`
  - Start backend: `python -m src.llm_council.ui_server`
  - The server serves `frontend/dist` automatically at `/` and assets at `/assets`.

If `frontend/dist` is missing, the server shows a short message with setup instructions at `/`.

## ‚úÖ Testing & Coverage

- Backend (pytest):
  - `pytest --maxfail=1 --disable-warnings -q`
  - With coverage: `pytest --cov=src/llm_council --cov-report=term-missing`

- Frontend (Vitest):
  - `cd frontend && npm run test`
  - With coverage/UI: `npm run test:coverage` or `npm run test:ui`

Targets: backend ‚â•85%, frontend ‚â•80%. See `docs/TEST_PLAN.md` and `docs/TRACEABILITY_MATRIX.md`.
