# RESEARCH_BRIEF.md — LLM Council Audit & Consensus Platform

**Owner (Research Lead):** Erik Cohen
**Date:** 2025-08-29  
**Status:** Draft  
**Prerequisites:** None (this is the starting point)  
**Next:** → [Market Scan](./MARKET_SCAN.md) → [Vision](./VISION.md)

## 1) Research Question & Hypothesis

**Core Question:** How can we systematically improve documentation quality and decision-making in software development using AI-assisted multi-perspective reviews?

**Hypothesis:** An ensemble of specialized LLM auditors can provide faster, more consistent, and comprehensive document reviews than traditional human-only processes, while maintaining quality through consensus mechanisms and alignment checks.

## 2) Problem Definition & Context

**Problem Space:**

- Documentation reviews are bottlenecks in development cycles
- Inconsistent review quality across teams and projects
- Important perspectives (security, scalability, cost) often missed
- Feedback lacks actionable specificity and traceability
- Scope creep due to unclear requirements and poor alignment

**Business Impact:**

- Delayed launches due to review cycles
- Technical debt from missed architectural issues
- Rework costs from misaligned requirements
- Security vulnerabilities from insufficient review coverage

## 3) Research Methodology

**Primary Research:**

- [ ] Survey 20+ engineering teams about doc review pain points
- [ ] Interview 5 senior engineers/architects about current processes
- [ ] Analyze 10 real project doc sets for common failure patterns

**Secondary Research:**

- [ ] Academic literature on automated code/doc review
- [ ] Industry reports on AI-assisted development tools
- [ ] Analysis of existing tools and their limitations

**User Research:**

- [ ] Shadow 3 teams through their doc review cycles
- [ ] Time/cost analysis of current manual review processes
- [ ] Quality assessment of outputs from different review approaches

## 4) Success Criteria for Research Phase

**Validation Criteria:**

- [ ] ≥3 credible sources confirm the problem exists at scale
- [ ] ≥5 alternative approaches identified and evaluated
- [ ] Clear differentiation from existing solutions articulated
- [ ] Quantified opportunity size (time saved, cost reduced, quality improved)
- [ ] Technical feasibility confirmed through small-scale tests

**Decision Framework:**

- **BUILD:** Unique solution, large market, technical feasibility high
- **BUY:** Existing solution meets 80%+ needs, cost-effective
- **PARTNER:** Complementary solution exists, integration viable
- **DEFER:** Problem not validated or solution not technically feasible

## 5) Key Research Questions

### Market & Competition

- [ ] What existing tools provide automated document review?
- [ ] How do current solutions handle multi-perspective analysis?
- [ ] What are the gaps in existing solutions?
- [ ] What is the total addressable market?

### Technical Feasibility

- [ ] Can LLMs provide consistent, high-quality document analysis?
- [ ] How do different models compare for structured feedback?
- [ ] What consensus mechanisms work best for document quality?
- [ ] How can we ensure alignment across document evolution?

### User Needs & Behaviors

- [ ] What are the most critical review perspectives needed?
- [ ] How do teams currently handle document approvals/gates?
- [ ] What level of automation vs. human oversight is desired?
- [ ] What output formats are most useful for different roles?

## 6) Resource Requirements

**Time:** 2-3 weeks for comprehensive research  
**Budget:** $500-1000 (survey tools, interview incentives, tool trials)  
**Personnel:** 1 researcher + domain expert interviews

## 7) Risk Assessment

**Research Risks:**

- Sample bias in user interviews
- Limited access to proprietary review processes
- Rapidly evolving AI tool landscape

**Mitigation:**

- Diverse interview pool across company sizes/industries
- Focus on process patterns vs. specific tools
- Regular research updates to capture new developments

## 8) Deliverables & Timeline

| Week | Deliverable                                  | Success Metric                         |
| ---- | -------------------------------------------- | -------------------------------------- |
| 1    | Problem validation & user interviews         | ≥10 stakeholder interviews completed   |
| 2    | Competitive analysis & technical feasibility | ≥5 alternative solutions analyzed      |
| 3    | Market opportunity & recommendation          | Clear BUILD/BUY/PARTNER/DEFER decision |

## 9) Research Findings (TBD)

_This section will be populated as research progresses_

### Problem Validation

- [ ] Problem confirmed by ≥80% of interviewed teams
- [ ] Quantified impact: avg X hours/week spent on reviews
- [ ] Current process satisfaction score: X/10

### Solution Validation

- [ ] Technical feasibility score: X/10
- [ ] Differentiation strength: X/10
- [ ] Market opportunity: $X TAM

### Final Recommendation

**Decision:** [BUILD/BUY/PARTNER/DEFER]  
**Rationale:** [Key factors influencing decision]  
**Next Steps:** [Specific actions based on decision]

### Gate checklist (Research → Market Scan)

- [ ] ≥10 stakeholder interviews completed with documented insights
- [ ] Problem validated by ≥80% of target users
- [ ] ≥3 credible sources confirm market opportunity
- [ ] Technical feasibility assessment shows ≥7/10 viability
- [ ] 0 **CRITICAL**, ≤2 **HIGH** blocking issues identified
